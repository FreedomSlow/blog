---
layout: post
title:  How to find a girlfriend (or a boyfriend) using Neural Networks 
date: 2023-12-19 22:25:00
description:
tags: neural-networks, python
categories: neural-networks, projects
tikzjax: true
---

Once, I had a long weekend with nothing to do. After bingewatching **Friends** for the 100th time 
I needed something to overcome the boredom. It's worth to mention that I was aggressively single back then,
so I registered on [Tinder](https://tinder.com), created profile and started swiping. That turned out
to be incredibly dull though, so my mind drifted off, and somehow I pressed `cmd+r` on my keyboard,
which, of course, made the page reload. What I found interesting, is that it took forever to load
the page; firstly, I blamed this on the german internet, but quick pinging to the closest CDN server
showed that my network speed was fine (at least by the german standards). 
Naturally, I had to understand what caused the pageload to be so slow. 

By the looks of it, web version of tinder is an [SPA](https://en.wikipedia.org/wiki/Single-page_application),
so it might load a ton of content, but what content is that exactly? Couple of images and some text
can't possibly take that long, even with all the modern frontend over-engineering.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/tinder_ai/tinder_preview.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>

</div>
<div class="caption">
    This is how tinder web interface looks like
</div>


Needless to say, I opened dev console and started looking at network requests. And one particular request has
caught my eye, it took the longest to execute, here it is

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/tinder_ai/console_blurred.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>

</div>
<div class="caption">
    API response
</div>

So to retrieve profiles info: photos, descriptions and many other fields, frontend makes an API call to
`api.gotinder.com/v2/recs/core` endpoint, which returns a full profile info not for one, but for more than 10 profiles
for one call. I reckon that's done to provide smoother user experience. You can see that photos are retrieved from
`images-ssl.gotinder.com` and they are publicly accessible, you just need to know the exact photo ID to retrieve them. 

To test this, I opened good old Postman and sent the test request, which to my excitement worked flawlessly. 
One thing that's worth to mention: each request requires an X-Auth token, which you can find in the same request
headers. Just copy this string and store it somewhere

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/tinder_ai/postman_blurred.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>

</div>
<div class="caption">
    Testing API with postman
</div>

**Note:** Token expires with the session, and while writing this, I'm not sure what is necessary to have the same session running

And now, obvious and attractive idea popped up into my head: if one can parse tinder that easily, the exhausting
task of finding love can be surely automized with just two Neural Networks, and I can go back to re-watching
all my favourite TV-series from the 00-s, not worrying about my love life anymore, while the heartless machine 
does all the work for me.

As I'm not paid hourly here, there's no need to create anything complex and over-engineered to sell it as "scalable and 
fault-tolerant", so I tried to keep the design pretty simple: one could easily run it locally or using VM with GPU

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/tinder_ai/architecture.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>

</div>
<div class="caption">
    Swipy architecture
</div>

I generally like to give projects names, it makes me care about them a little bit more, let's call
this one Swipy.

## Get user profiles from tinder
To parse the data we need to write a simple API interface that will get all the profile meta info. But first, let's
take a closer look at the API response and try to get all the fields that might help us find the significant other.
I found useful:
- user_id
- distance to the user in miles
- name
- bio - text description
- birthdate
- gender
- job description
- education description

And here is the `UserProfile` class that will store all of this, there is also a helper `bio_to_dict` method
that will be useful later. Btw you can find all the source code [here](https://github.com/FreedomSlow/tinder_swiper)):
```python
class UserProfile:
    def __init__(self, data, api: tinderAPI):
        self.api = api
        self._parse_data(data)

    def _parse_data(self, data):
        self.id = data["_id"]
        # Convert miles to units that normal people use
        self.distance = data.get("distance_mi", 0) / KM_IN_MILES

        self.name = data.get("name", "Unknown")
        self.photos = list(map(lambda photo: photo["url"], data.get("photos", [])))

        self.bio = data.get("bio", "")

        if data.get("birth_date", False):
            self.birth_date = datetime.datetime.strptime(
                data["birth_date"],
                "%Y-%m-%dT%H:%M:%S.%fZ"
            )
        else:
            self.birth_date = None

        self.gender = ["Male", "Female", "Unknown"][data.get("gender", 2)]
        self.jobs = list(
            map(
                lambda job: {
                    "title": job.get("title", {}).get("name"),
                    "company": job.get("company", {}).get("name")},
                data.get("jobs", [])
            )
        )
        self.schools = list(map(lambda school: school["name"], data.get("schools", [])))

    def bio_to_dict(self):
        return {
            "name": self.name,
            "bio": self.bio,
            "age": int(relativedelta(datetime.datetime.today(), self.birth_date).years)
        }
```
Attentive reader might notice that `UserProfile` constructor expects `tinderAPI` - it is the API interface class.
For now, it'll have only methods to retrieve profiles, but we'll extend it later:
```python
class tinderAPI:
    def __init__(self, token: str, api_url: str = "https://api.gotinder.com"):
        self._token = token
        self.api_url = api_url

    def create_profile(self):
        data = requests.get(
            f"{self.api_url}/v2/profile?include=account%2Cuser",
            headers={"X-Auth-Token": self._token}
        ).json()
        return UserProfile(data["data"], self)

    def get_profiles(self):
        data = requests.get(
            f"{self.api_url}/v2/recs/core", 
            headers={"X-Auth-Token": self._token}
        ).json()
        return list(map(lambda user: UserProfile(user["user"], self), data["data"]["results"]))
```
It's a known fact, that we decide whether we like the person or not in the first 15 seconds that we see them. So for
Swipy to be as superficial and ignorant as we are, we have to retrieve the photos as well. Let's extend `UserProfile`:
```python
def download_photos(self, image_dir: str, sleep_for: float = 0.5, already_downloaded: set = None):
    if already_downloaded is not None and self.id in already_downloaded:
        return already_downloaded
    if already_downloaded is None:
        already_downloaded = {self.id}
    else:
        already_downloaded.add(self.id)

    for ind, image_url in enumerate(self.photos):
        req = requests.get(image_url, stream=True)
        if req.status_code == 200:
            with open(f"{image_dir}/{self.id}_{self.name}_{ind}.jpeg", "wb") as f:
                f.write(req.content)

        # Necessary to aviod the ban-hammer
        sleep(random() * sleep_for)

    return already_downloaded
```
As one profile might (and probably should) have more than one photo, we will index them before saving.
Also, recommendation feed can have the same profiles, if we haven't liked/disliked them yet, hence there's 
`already_downloaded` state, that will help us to save some time and traffic

**Note:** Yeah, I know, this should be written with async, but wcyd :)

And it seems like we have everything ready to start downloading profiles:
```python
api = tinderAPI(args.token)

logger.info("Start parsing profiles")

already_downloaded = read_pickle(profiles_path)
bios = read_pickle(bios_path, {})
for _ in range(args.num_requests):
    profiles = api.get_profiles()
    for profile in tqdm(profiles):
        already_downloaded = profile.download_photos(image_dir, already_downloaded=already_downloaded)
        sleep(random() * 5)
        bios[profile.id] = profile.bio_to_dict()

logger.info("Saving parsed profiles and bios")
with open(bios_path, "wb") as f:
    pickle.dump(bios, f)
with open(profiles_path, "wb") as f:
    pickle.dump(already_downloaded, f)

logger.info("All done!")
```

While images are slowly but surely downloading, it's time to think about data labeling: as I intend to classify
potential partners with NN-s, I have to provide the training data for it. Unfortunately, I can't outsource this
particular data labeling problem to some strangers on the internet (or can I? Here's an idea for the project, 
but I think Zuckerberg [has done something similar](https://en.wikipedia.org/wiki/History_of_Facebook#FaceMash) to it already).
That means that we have to built some sort of app to do the labeling, as I'm a bit sick of frontend development at the moment,
I'll build a simple desktop app, letting [Tkinter](https://docs.python.org/3/library/tkinter.html) to do all the heavy
lifting for me.

The app is pretty straightforward, we just need to display: name, age, bio and photos; add photo scrolling feature
and the ability to like and dislike the profile. Even though I'm not really proficient with tkinter, this sounds like
a job that even I would be able to do. After an hour or so, without any help from ChatGPT, I was able to produce
something that worked.

First, we need to create a canvas, and add some elements to it
```python
window = tkinter.Tk()
window.geometry("720x1080")

img_lbl = tkinter.Label(window)
img_lbl.pack(anchor="center")

name_age = tkinter.Label(window)
name_age.place(relx=0.5, rely=0.8, anchor="center")

bio_text = tkinter.Label(window)
bio_text.place(relx=0.5, rely=0.85, anchor="center")

btn_like = tkinter.Button(window, text="Like", fg="green", command=positive)
btn_like.place(relx=0.7, rely=0.9, anchor="center")

btn_dislike = tkinter.Button(window, text="Dislike", fg="red", command=negative)
btn_dislike.place(relx=0.3, rely=0.9, anchor="center")

btn_prev = tkinter.Button(window, text="Prev Image", command=prev_image)
btn_prev.place(relx=0.3, rely=0.95, anchor="center")

btn_next = tkinter.Button(window, text="Next Image", command=next_image)
btn_next.place(relx=0.7, rely=0.95, anchor="center")
```
here you can see three event handlers: `positive`, `negative`, `prev_image` and `next_image`, here is how they look like:
```python
def positive():
    global current_profile
    labels_[current_profile] = 1
    next_profile()


def negative():
    global current_profile
    labels_[current_profile] = 0
    next_profile()


def next_image():
    global cnt, profile_images
    if cnt < len(profile_images):
        cnt = min(len(profile_images) - 1, cnt + 1)
        display_image(os.path.join(IMAGES_DIR, profile_images[cnt]))
    else:
        pass


def prev_image():
    global cnt, profile_images
    if cnt >= 0:
        cnt = max(0, cnt - 1)
        display_image(os.path.join(IMAGES_DIR, profile_images[cnt]))
    else:
        pass
```
As the state we keep track of the current profile, images in that profile and the image that is currently displayed

**Note:** I know that `global` is evil, but it was already 1AM and I didn't want to spend another hour researching how 
to manage the state properly. So give me a break, will you

Next, we need to display profile images and bio:
```python
def display_image(path: str):
    img = Image.open(path)
    width, height = img.size
    max_height = 800
    if height > max_height:
        resize_factor = max_height / height
        img = img.resize((int(width * resize_factor), int(height * resize_factor)), resample=Image.LANCZOS)
    img_tk = ImageTk.PhotoImage(img)
    img_lbl.img = img_tk
    img_lbl.config(image=img_lbl.img)


def display_bio(profile_id: str):
    try:
        bio_text.config(text=make_pretty_text([profile_id]["bio"]))
        name_age.config(
            text=f"{bios[profile_id]['name']}: {bios[profile_id]['age']}",
            font=("Helvetica", 32)
        )
    except KeyError:
        bio_text.config(text="No bio available")
```

And finally, let's add the logic to display next profile as the current one has already been labeled:
```python
def next_profile():
    global cnt, current_profile, profile_images
    cnt = -1

    try:
        current_profile = next(not_labeled_profiles)
    except StopIteration:
        print("All images are labeled! Closing the app")
        on_close()
    profile_images = list(filter(lambda path: path.startswith(current_profile), all_images_))
    next_image()
    display_bio(current_profile)
```

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/tinder_ai/label_app.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>

</div>
<div class="caption">
    Labeling app
</div>

The app doesn't look particularly good, but it does the job, so I'm more than satisfied with it. One thing that we need to add
though, is a handler that will save labeled profiles when the app is closed. Because labels are saved during runtime
and not written anywhere yet.
```python
def on_close():
    with open(LABELS_PATH, "wb") as f:
        pickle.dump(labels_, f)
        window.destroy()

# Add this to the canvas definition
window.protocol("WM_DELETE_WINDOW", on_close)
window.mainloop()
```

After collecting and labeling somewhere around five hundred profiles (was pretty exhausting actually), we are now ready
for the exciting stuff!

## Image classifier
As a backbone to our robotic eyes I chose [ResNet](https://arxiv.org/abs/1512.03385) - it's relatively cheap, easy and 
quick to train, and it's not that much code either, which at this point was a good thing, cause things have already started
to look a bit blurry on the edges.

ResNet is build of ResBlocks, here it is implemented in pytorch:
```python
class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, padding=1):
        super().__init__()

        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=False)
        self.bn_1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn_2 = nn.BatchNorm2d(out_channels)

        self.res_con = None
        if stride != 1 and in_channels != out_channels:
            self.res_con = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, X):
        input_ = X
        out = self.bn_1(self.conv_1(X))
        out = self.relu(out)
        out = self.bn_2(self.conv_2(out))

        if self.res_con is not None:
            out += self.res_con(input_)

        out = self.relu(out)
        return out
```
You can stack ResBlocks and play with channel configurations to get deeper and wider model. Here is an implementation
that can support arbitrary parameters
```python
class ResNet(nn.Module):
    def __init__(self, input_channels, out_channels, layers, num_classes=10):
        self.hidden_size = out_channels

        super().__init__()
        self.conv_1 = nn.Conv2d(
            input_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False
        )
        self.bn_1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)

        self.res_1 = self.make_res_block(out_channels, out_channels, layers[0], 1)
        self.res_2 = self.make_res_block(out_channels, 128, layers[1], 2)
        self.res_3 = self.make_res_block(128, 256, layers[2], 2)
        self.res_4 = self.make_res_block(256, 512, layers[3], 2)

        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.flat = nn.Flatten()
        self.out = nn.Linear(512, num_classes, bias=False)

    def make_res_block(self, input_channels, output_channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(ResBlock(self.hidden_size, output_channels, stride))
            self.hidden_size = output_channels

        return nn.Sequential(*layers)

    def forward(self, X):
        out = self.bn_1(self.conv_1(X))
        out = self.relu(out)

        out = self.res_1(out)
        out = self.res_2(out)
        out = self.res_3(out)
        out = self.res_4(out)

        out = self.avg_pool(out)
        out = self.flat(out)
        out = self.out(out)
        
        return out
```
I'll opt out for regular ResNet-50 though. Btw, you can try to fine-tune the pretrained model, e.g. the one that
was trained [ImageNet](https://huggingface.co/microsoft/resnet-50) or on face-recognition task. We also need a
[DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) with our images
```python
def load_images(
    data_path: str,
    batch_size: int,
    shuffle: bool = True,
    num_workers: int = 1,
    transformations: list = None,
    train_size: float = None,
    seed: int = 42
):
    transforms = torchvision.transforms.Compose(transformations) if transformations else None
    data = torchvision.datasets.ImageFolder(data_path, transforms)

    if train_size is not None:
        _train_size = int(len(data) * train_size)
        _val_size = len(data) - _train_size

        print("Train dataset size:", _train_size, "test dataset size:", _val_size)

        gen = torch.Generator().manual_seed(seed)
        data_train, data_val = random_split(data, [_train_size, _val_size], generator=gen)

        return (
            DataLoader(data_train, batch_size, shuffle, num_workers=num_workers),
            DataLoader(data_val, batch_size, shuffle, num_workers=num_workers)
        )

    return DataLoader(data, batch_size, shuffle, num_workers=num_workers)
```

I trained the model for 100 epochs
using [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW) optimiser,
with initial learning rate of 0.1 with `CosineAnnealingLR` scheduler and weight decay. I skip boring stuff like
some utility functions and training loop, but if you are interested, you can find it [here](https://github.com/FreedomSlow/tinder_swiper/blob/master/image_classifier.py)

Swipy can see, but I'd like to think that there's something else to the person apart from the looks,
so now it's time to teach it read.

## Profile description classifier
Inevitably we'll use some sort of [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)),
but as I have only 50 euros on my AWS account, and I'm not ready to completely sell my soul to Besos, let's
pick something pretrained and relatively small, so we can fine tune it. The only requirement for the model is to
be multilingual - I live in Berlin and people here can speak almost any language, 
most commonly English and German though. I picked [this](https://huggingface.co/distilbert-base-multilingual-cased) 
Distilbert, which comes with pretrained tokenizer. Distillation is a process of training a small student model, 
that can imitate the big and expensive teacher model. 
That results in a bit worse performance, but allows to significantly speed up training and inference.

Tinder users utilize various emojis quite heavily. While this adds a nice emotional touch, it will most likely
confuse the model, because tokenizer will treat this as unknown token, so I, unfortunately, have to remove
them, as well some other unicode characters
```python
def clean_bios(dataset, bio_col: str):
    """Remove flags, emojis and special symbols from profiles"""
    def _cleaner(text):
        flags = re.findall(u"[\U0001F1E6-\U0001F1FF]", text)

        text = text.replace("\n", " ")
        text = "".join(w for w in text if w not in emoji.EMOJI_DATA and w not in flags)
        text = text.replace("\u200d", "")

        return text

    return {f"{bio_col}_clean": _cleaner(dataset[bio_col])}
```

Now let's load, clean and tokenize our texts
```python
dataset_dict = defaultdict(list)
unlabeled = []

for k, v in bios.items():
    if k in labels:
        dataset_dict["id"].append(k)
        dataset_dict["label"].append(labels[k])
        for k_, v_ in v.items():
            dataset_dict[k_].append(v_)
    else:
        unlabeled.append(k)

logger.info(f"There are {len(unlabeled)} unlabeled profiles")
logger.info(f"Load and prep dataset with {args.test_size} test size")

bios_dataset = Dataset.from_dict(dataset_dict)
tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)
bios_dataset = prep_data(bios_dataset, tokenizer)
bios_dataset = bios_dataset.train_test_split(test_size=args.test_size)
```

I also freeze most of the layers - this will speed up the training significantly
```python
layers_to_train = [
        "distilbert.transformer.layer.5.ffn.lin1.weight",
        "distilbert.transformer.layer.5.ffn.lin1.bias",
        "distilbert.transformer.layer.5.ffn.lin2.weight",
        "distilbert.transformer.layer.5.ffn.lin2.bias",
        "distilbert.transformer.layer.5.output_layer_norm.weight",
        "distilbert.transformer.layer.5.output_layer_norm.bias",
        "pre_classifier.weight",
        "pre_classifier.bias",
        "classifier.weight",
        "classifier.bias",
    ]
for n, l in bert.named_parameters():
    if n not in layers_to_train:
        l.requires_grad = False
```

I fine tune the model for 12,500 steps with the same AdamW optimiser and `5e-4` learning rate with `constant_with_warmup`
scheduler.

And I think now we have everything that we need to find love, without almost any effort.

## Making predictions and swiping
First thing first, we need to extend our `UserProfile` and `tinderAPI` with like and dislike functionality. 
Luckily this can be done using the same endpoint
```python
def like(self, user_id):
    data = requests.get(
        f"{self.api_url}/like/{user_id}",
        headers={"X-Auth-Token": self._token}
    ).json()
    return {
        "match": data["match"],
        "likes_remaining": data["likes_remaining"]
    }

def dislike(self, user_id: str):
    try:
        requests.get(
            f"{self.api_url}/pass/{user_id}",
            headers={"X-Auth-Token": self._token}
        ).json()
    except Exception as err:
        print(err)
        return False
    return True
```
Let's also retrieve matches that we have so far
```python
def get_matches(self, limit: int = 10):
    data = requests.get(
        f"{self.api_url}/v2/matches?count={limit}",
        headers={"X-Auth-Token": self._token}
    ).json()
    return list(map(lambda match: UserProfile(match["person"], self), data["data"]["matches"]))
```

I also want to be able to control Swipy's vanity, so I write the function that combines images and text scores
from models according to the weights
```python
def make_prediction(
    text_preds: np.ndarray,
    image_preds: np.ndarray,
    text_weight: float,
    image_weight: float,
    threshold: float = 0.5
):
    total_pred = text_preds * text_weight + image_preds * image_weight
    return np.where(total_pred > threshold, True, False)
```

And here is a part of prediction pipeline, I again skip over implementation details that I find tedious, you can find
complete inference code [here](https://github.com/FreedomSlow/tinder_swiper/blob/master/predict.py)
```python
# Text
text_dataset = Dataset.from_dict(profile_info)
text_dataset = prep_data(text_dataset, tokenizer, cols_to_drop=["id"])
text_preds = np.max(trainer.predict(text_dataset).predictions, axis=-1)

# Images
images_dataset = load_images(args.image_dir, 8, shuffle=False, transformations=[torchvision.transforms.ToTensor()])
image_preds = []
image_model.eval()
with torch.no_grad():
    for X, _ in images_dataset:
        image_preds.append(F.softmax(image_model(X.to(device)).detach().numpy()))
image_preds = np.max(np.concatenate(image_preds), axis=-1)

# Total predictions
preds = make_prediction(text_preds, args.bios_weight, image_preds, args.image_weight)

# Preds are ordered by id, so we like them if the entry is true in preds
liked = 0
for user_id, to_like in zip(profile_info, preds):
    if to_like:
        api.like(user_id)
        liked += 1

logger.info(f"Liked: {liked} profiles, disliked: {len(preds) - liked}")
```
Now I can peacefully work, sleep, ski, in a word - do whatever I want, knowing that Swipy will work tirelessly
until it finds my future wife or AWS account runs out of money, whichever comes first...

[Source code](https://github.com/FreedomSlow/tinder_swiper)

**Note:** Everything here was done for fun, education and so on, tinder, please, don't sue me  

**P.S.** If you read this and thought that it was kinda cool, and somehow happen to be a girl - hit me up, who knows,
I might still be single ;)

